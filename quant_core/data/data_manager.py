# -*- coding: utf-8 -*-
import pandas as pd
import os
import time
import duckdb
from datetime import datetime
from .engine.us_equity_engine import USEquityEngine
# [æ–°å¢] å¯¼å…¥ benchmark åŒæ­¥å‡½æ•°
from .engine.benchmark_engine import sync_benchmarks

class DataManager:
    """
    å·¥ä¸šçº§æ•°æ®ç®¡ç†å™¨ï¼šè´Ÿè´£èµ„äº§è°ƒåº¦ã€å¢é‡åŒæ­¥ã€æ•°æ®æŒä¹…åŒ–åŠè´¨é‡å®¡è®¡ã€‚
    """
    def __init__(self, ib_client=None):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨ã€‚
        :param ib_client: å·²è¿æ¥çš„ IB å®¢æˆ·ç«¯å®ä¾‹ã€‚
        """
        self.ib = ib_client
        # è·¯å¾„é…ç½®ï¼šå¯¹é½é¡¹ç›®æ ‡å‡†ç›®å½•ç»“æ„
        self.ref_path = 'data/reference/sec_code_category_grouped.csv'
        self.storage_path = 'data/processed/all_price_data.parquet'
        
        # å¼•æ“é…ç½®ï¼šä»…åœ¨éœ€è¦åŒæ­¥æ—¶åˆå§‹åŒ–ç¾è‚¡å¼•æ“
        self.us_engine = USEquityEngine(self.ib) if self.ib else None
        
        # å­—æ®µå¸ƒå±€ï¼šä¸¥æ ¼æ‰§è¡Œè¦æ±‚çš„åˆ—é¡ºåº
        self.columns_layout = [
            'id', 'datetime', 'sec_code', 'category_id', 'pre_close', 'open', 'high', 'low', 'close', 
            'volume', 'amount', 'create_time', 'avg_price', 'simple_return', 
            'shares_outstanding', 'turnover', 'market_cap'
        ]

    def run_pipeline(self, sync=True, check=True, duration='15 Y'):
        """
        æ•°æ®æµæ°´çº¿å”¯ä¸€å…¥å£ã€‚
        """
        print("="*60)
        print(f"ğŸš€ æ•°æ®æµæ°´çº¿å¯åŠ¨ | å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)

        if sync:
            # 1. æ‰§è¡ŒåŸæœ‰çš„ç¾è‚¡åŒæ­¥ (Equities)
            self._execute_sync(default_duration=duration)
            
            # [æ–°å¢] 2. æ‰§è¡ŒåŸºå‡†åŒæ­¥ (Benchmarks)
            # æ”¾åœ¨è‚¡ç¥¨åŒæ­¥ä¹‹åï¼Œç¡®ä¿é€»è¾‘è§£è€¦
            print("\nğŸ“¡ æ­¥éª¤ 1.5: åŒæ­¥åŸºå‡†æ•°æ® (Benchmarks)...")
            try:
                sync_benchmarks(self.ib)
            except Exception as e:
                print(f"âš ï¸ åŸºå‡†æ•°æ®åŒæ­¥å¤±è´¥: {e}")
        
        if check:
            self._execute_quality_check()
            
        print("="*60)
        print("âœ¨ æµæ°´çº¿æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ã€‚")
        print("="*60)

    def _get_last_sync_info(self):
        """
        åˆ©ç”¨ DuckDB æé€Ÿè·å–æœ¬åœ°æ–‡ä»¶çš„åŒæ­¥è¿›åº¦ã€‚
        """
        if not os.path.exists(self.storage_path):
            return {}
        
        con = duckdb.connect()
        try:
            # æ‰«æ Parquet æ–‡ä»¶è·å–æ¯ä¸ªæ ‡çš„çš„æœ€æ–°æ—¥æœŸ
            df_last = con.execute(f"""
                SELECT sec_code, max(datetime) as last_date 
                FROM '{self.storage_path}' 
                GROUP BY sec_code
            """).df()
            return dict(zip(df_last['sec_code'], df_last['last_date']))
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–æœ¬åœ°åŒæ­¥è¿›åº¦: {e}")
            return {}
        finally:
            con.close()

    def _execute_sync(self, default_duration):
        """
        æ ¸å¿ƒåŒæ­¥é€»è¾‘ï¼šæ”¯æŒæ™ºèƒ½è·³è¿‡ä¸å¢é‡è¡¥å…¨ã€‚
        """
        print("ğŸ“¡ æ­¥éª¤ 1: æ£€æŸ¥æœ¬åœ°è¿›åº¦å¹¶æ‰§è¡Œæ™ºèƒ½åŒæ­¥...")
        if not self.ib or not self.ib.isConnected():
            raise RuntimeError("âŒ é”™è¯¯: æ‰§è¡ŒåŒæ­¥éœ€è¦æœ‰æ•ˆçš„ IBKR è¿æ¥ã€‚")

        # 1. è·å–æœ¬åœ°æ¯ä¸ªæ ‡çš„çš„æœ€åæ—¥æœŸåŠå…¨å±€æœ€æ™šæ—¥æœŸ
        last_dates = self._get_last_sync_info()
        global_max_date = max(last_dates.values()) if last_dates else None
        
        if global_max_date:
            print(f"ğŸ“Š æ•°æ®åº“å…¨å±€æœ€æ–°æ—¥æœŸ: {global_max_date.date()}")

        # 2. åŠ è½½èµ„äº§æ± æ¸…å•
        if not os.path.exists(self.ref_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°èµ„äº§æ¸…å•æ–‡ä»¶: {self.ref_path}")
            
        universe_df = pd.read_csv(self.ref_path)
        cat_col = 'universe' if 'universe' in universe_df.columns else 'category_id'
        
        new_data_list = []
        today = datetime.now()
        total_tickers = len(universe_df)

        # 3. éå†èµ„äº§æ± æ‰§è¡Œå¢é‡æ‹‰å–
        for i, row in universe_df.iterrows():
            symbol = row['sec_code']
            category = row[cat_col]
            last_date = last_dates.get(symbol)
            
            # --- æ™ºèƒ½è·³è¿‡é€»è¾‘ï¼šå¦‚æœå·²è¿½å¹³å…¨å±€è¿›åº¦ä¸”å½“å‰éäº¤æ˜“æ—¶æ®µï¼Œåˆ™è·³è¿‡ ---
            if global_max_date and last_date == global_max_date:
                print(f"[{i+1}/{total_tickers}] â© {symbol} å·²æ˜¯å…¨å±€æœ€æ–° ({last_date.date()})ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
                continue
            
            # --- åŠ¨æ€ä¸‹è½½æ—¶é•¿è®¡ç®— ---
            if last_date:
                # è‡³å°‘å– 2 å¤©ä»¥ç¡®ä¿æ•°æ®è¡”æ¥
                days_diff = (today - last_date).days
                fetch_duration = f"{min(days_diff + 2, 365)} D" 
                print(f"[{i+1}/{total_tickers}] ğŸ“¥ {symbol} å¢é‡æ‹‰å–: è‡ª {last_date.date()} èµ·çš„ {fetch_duration} æ•°æ®...")
            else:
                fetch_duration = default_duration
                print(f"[{i+1}/{total_tickers}] ğŸ†• {symbol} é¦–æ¬¡å…¨é‡æ‹‰å–: {fetch_duration}...")
            
            try:
                # è°ƒç”¨ä¸“ç”¨å¼•æ“æ‰§è¡Œä¸‹è½½ä¸åˆçº§è®¡ç®—
                data = self.us_engine.fetch_data(symbol, category, duration=fetch_duration)
                if not data.empty:
                    new_data_list.append(data)
            except Exception as e:
                print(f"âš ï¸ ä¸‹è½½ {symbol} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            
            # é¢‘ç‡æ§åˆ¶ï¼šé˜²æ­¢è§¦å‘ IBKR Pacing Violation
            time.sleep(1.2)

        # 4. æ‰§è¡Œæ•°æ®åˆå¹¶ä¸æŒä¹…åŒ–
        if new_data_list:
            self._merge_and_save(new_data_list)
        else:
            print("âœ… æ£€æŸ¥å®Œæ¯•ï¼šæ‰€æœ‰èµ„äº§æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")

    def _merge_and_save(self, new_data_list):
        """
        åˆå¹¶æ–°æ—§æ•°æ®ï¼Œæ‰§è¡Œå»é‡å¹¶å›ºåŒ–ä¸º Parquetã€‚
        """
        print("ğŸ’¾ æ­£åœ¨åˆå¹¶æ•°æ®å¹¶æ›´æ–°æœ¬åœ° Parquet ä»“åº“...")
        new_df = pd.concat(new_data_list)
        
        if os.path.exists(self.storage_path):
            old_df = pd.read_parquet(self.storage_path)
            combined_df = pd.concat([old_df, new_df])
        else:
            combined_df = new_df

        # æ•°æ®å»é‡ï¼šåŸºäºæ—¶é—´å’Œä»£ç ç¡®ä¿æ•°æ®å”¯ä¸€æ€§
        combined_df = combined_df.drop_duplicates(subset=['datetime', 'sec_code'], keep='last')
        combined_df = combined_df.sort_values(['datetime', 'sec_code'])
        
        # é‡æ–°ç”Ÿæˆå…¨å±€å”¯ä¸€è‡ªå¢ ID
        if 'id' in combined_df.columns:
            combined_df = combined_df.drop(columns=['id'])
        combined_df.insert(0, 'id', range(8000000, 8000000 + len(combined_df)))

        # å†™å…¥ Parquet
        os.makedirs(os.path.dirname(self.storage_path), exist_ok=True)
        combined_df[self.columns_layout].to_parquet(self.storage_path, index=False, compression='snappy')
        print(f"âœ… æ›´æ–°æˆåŠŸï¼šæ•°æ®åº“å½“å‰æ€»è®°å½•æ•°: {len(combined_df):,}")

    def _execute_quality_check(self):
        """
        åˆ©ç”¨ DuckDB å®¡è®¡æ•°æ®è´¨é‡ï¼Œç¡®ä¿æ— ç©ºå€¼ä¸æ•°æ®æ–­å±‚ã€‚
        """
        print("\nğŸ” æ­¥éª¤ 2: å¼€å§‹æ•°æ®è´¨é‡è‡ªåŠ¨å®¡è®¡...")
        if not os.path.exists(self.storage_path):
            print(f"âŒ å®¡è®¡ç»ˆæ­¢ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {self.storage_path}")
            return

        con = duckdb.connect()
        try:
            # åŸºç¡€åˆ†å¸ƒå®¡è®¡
            res = con.execute(f"""
                SELECT count(*) as rows, count(distinct sec_code) as tickers,
                       min(datetime) as start_v, max(datetime) as end_v
                FROM '{self.storage_path}'
            """).df()
            
            # å…³é”®å­—æ®µç©ºå€¼å®¡è®¡
            nulls = con.execute(f"SELECT count(*) FROM '{self.storage_path}' WHERE close IS NULL").fetchone()[0]
            
            print(f"- æ•°æ®è¡Œæ•°: {res['rows'][0]:,}")
            print(f"- æ ‡çš„ä¸ªæ•°: {res['tickers'][0]}")
            print(f"- æ—¥æœŸè¦†ç›–: {res['start_v'][0]} è‡³ {res['end_v'][0]}")
            
            if nulls == 0:
                print("ğŸ’ è´¨é‡ç»“è®º: æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±å­—æ®µã€‚")
            else:
                print(f"âš ï¸ è´¨é‡é¢„è­¦: å‘ç° {nulls} æ¡ç¼ºå¤±è®°å½•ï¼Œè¯·æ ¸æŸ¥æ•°æ®æºï¼")
                
        except Exception as e:
            print(f"âŒ å®¡è®¡è¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            con.close()